{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"image_processing\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.catalog.myCatalog\", \"com.datastax.spark.connector.datasource.CassandraCatalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_path = \"/Users/lavanyamk/Documents/FeaViz/object_data/\"\n",
    "model_label_path = \"/Users/lavanyamk/Documents/FeaViz/coco-yolo-v3/coco.names\"\n",
    "model_weights_path = \"/Users/lavanyamk/Documents/FeaViz/coco-yolo-v3/yolov3.weights\"\n",
    "model_cfg_path = \"/Users/lavanyamk/Documents/FeaViz/coco-yolo-v3/yolov3.cfg\"\n",
    "http_server_ip = \"http://localhost:9753\"\n",
    "\n",
    "# db_bundle_path = \"/Users/lavanyamk/Downloads/secure-connect-nosql-db.zip\"\n",
    "# user_name = \"CgYxKnkrerLwiDCMYlNXdPHa\"\n",
    "# password = \"91YcRxuTwnZR-.WB-TJa5HOoFL8oL,AdmQusxObsSK5pIriJM0kFX9oSgr1jj3BakDZ5jOz4AZy0rpOwT76JWUNQWTC8u1an5FmJs-FR1xsczTLxJXFGUZczuGCMJ9aK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_df = spark.read.format(\"image\")\\\n",
    "    .option(\"basePath\", img_data_path)\\\n",
    "    .option(\"dropInvalid\", True).load(\"file://\" + img_data_path + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening file, reading, eliminating whitespaces, and splitting by '\\n', which in turn creates list\n",
    "labels = open(model_label_path).read().strip().split('\\n')  # list of names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting minimum probability to eliminate weak predictions\n",
    "probability_minimum = 0.6\n",
    "\n",
    "# Setting threshold for non maximum suppression\n",
    "threshold = 0.5\n",
    "\n",
    "network = cv2.dnn.readNetFromDarknet(model_cfg_path, model_weights_path)\n",
    "\n",
    "# Getting names of all layers\n",
    "layers_names_all = network.getLayerNames()  # list of layers' names\n",
    "\n",
    "# # Check point\n",
    "# print(layers_names_all)\n",
    "\n",
    "# Getting only output layers' names that we need from YOLO algorithm\n",
    "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]  # list of layers' names\n",
    "\n",
    "# Check point\n",
    "# print(layers_names_output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc(img, labels):\n",
    "    image = cv2.imread(img.split('//')[-1])\n",
    "    image_input_shape = image.shape\n",
    "    network = cv2.dnn.readNetFromDarknet(model_cfg_path, model_weights_path)\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    layers_names_all = network.getLayerNames()\n",
    "    layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()] \n",
    "    network.setInput(blob)  # setting blob as input to the network\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    h, w = image_input_shape[:2]\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "    class_labels = []\n",
    "    for result in output_from_network:\n",
    "        for detection in result:\n",
    "            scores = detection[5:]\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting confidence (probability) for current object\n",
    "            confidence_current = scores[class_current]\n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial image size\n",
    "                # YOLO data format keeps center of detected box and its width and height\n",
    "                # That is why we can just elementwise multiply them to the width and height of the image\n",
    "                box_current = detection[0:4] * np.array([w, h, w, h])\n",
    "                # From current box with YOLO format getting top left corner coordinates\n",
    "                # that are x_min and y_min\n",
    "                x_center, y_center, box_width, box_height = box_current.astype('int')\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(int(class_current))\n",
    "                class_labels.append(labels[class_current])\n",
    "    return Row(#\"bounding_box\", \n",
    "               \"confidence_score\",\n",
    "               \"class\",\n",
    "               \"class_labels\")(#bounding_boxes,\n",
    "                               confidences,\n",
    "                               class_numbers, \n",
    "                               class_labels)\n",
    "\n",
    "schema_added = StructType([\n",
    "#     StructField(\"bounding_box\", ArrayType(ArrayType(IntegerType())), False),\n",
    "    StructField(\"confidence_score\", ArrayType(FloatType()), False),\n",
    "    StructField(\"classes\", ArrayType(IntegerType()), False),\n",
    "    StructField(\"class_labels\", ArrayType(StringType()), False)\n",
    "])\n",
    "\n",
    "udf_image = udf(lambda x: get_desc(x, labels), schema_added)\n",
    "\n",
    "\n",
    "features_df = img_features_df.select(\"image.origin\", \"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\")\\\n",
    "    .withColumnRenamed(\"origin\", \"image_path\")\n",
    "features_df = features_df.withColumn(\"desc\", udf_image(\"image_path\")).select( \"*\", \"desc.*\").drop(\"desc\")#.show()\n",
    "image_features_df = features_df.withColumn(\"image_view\", concat(lit('<img src=\"'+http_server_ip),\n",
    "                                             split(col(\"image_path\"), img_data_path).getItem(1), \n",
    "                                             lit('\"  width=\"150\" height=\"200\">')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_features_df.write.mode(\"append\").partitionBy(\"image_path\").saveAsTable(\"myCatalog.nosql1.image_features_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sample_df.show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/veggies_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/broccoli_1.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/apple_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/veggies_1.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/berry_1.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/carrot_2.jpg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/apple_1.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/orange_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/banana-red_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/broccoli_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/bnana_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/carrot.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/berry_2.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/bana.jpeg\" alt=\"local_image\" />'],\n",
       "       ['<img src=\"file:///Users/lavanyamk/Documents/FeaViz/object_data/orange_1.jpeg\" alt=\"local_image\" />']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image_features_df.select(\"image_view\").toPandas().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"http://localhost:9753/veggies_2.jpeg\" width=\"150\" \n",
    "     height=\"200\">\n",
    "<!-- <img src=\"/images/picture.jpg\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features_df.coalesce(1).write.option(\n",
    "#     \"header\",True).mode('overwrite').parquet(\"file:///Users/lavanyamk/Documents/FeaViz/outputs/image_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple([str(m[1]) for m in features_df.dtypes]), features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://www.datasciencemadesimple.com/count-of-missing-nanna-and-null-values-in-pyspark/\n",
    "# spark.createDataFrame(features_df.dtypes).show()\n",
    "\n",
    "# spark.createDataFrame([(\"data_type\")] + [(m[1]) for m in features_df.dtypes], [\"summary\"] + features_df.columns)\n",
    "# rdd.toDF(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "|             summary|top_25|top_50|top_75|count|data_type|distinct_count|                 max|              mean|                 min|missing_count|            stddev|\n",
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "|  product_photos_qty|   1.0|   1.0|   3.0|32341|   double|            19|                20.0|2.1889861166939797|                 1.0|          610|1.7367656379315435|\n",
      "|product_category_...|      |      |      |32341|   string|            73|       watches_gifts|                  |agro_industry_and...|          610|                  |\n",
      "|    product_weight_g| 300.0| 700.0|1900.0|32949|   double|          2204|             40425.0|2276.4724877841513|                 0.0|            2| 4282.038730977024|\n",
      "|    product_width_cm|  15.0|  20.0|  30.0|32949|   double|            95|               118.0|23.196728277034204|                 6.0|            2|12.079047453227794|\n",
      "| product_name_lenght|  42.0|  51.0|  57.0|32341|   double|            66|                76.0| 48.47694876472589|                 5.0|          610|10.245740725237287|\n",
      "|   product_height_cm|   8.0|  13.0|  21.0|32949|   double|           102|               105.0|16.937661234028347|                 2.0|            2|13.637554061749569|\n",
      "|   product_length_cm|  18.0|  25.0|  38.0|32949|   double|            99|               105.0| 30.81507784758263|                 7.0|            2|16.914458054065953|\n",
      "|product_descripti...| 339.0| 595.0| 972.0|32341|   double|          2960|              3992.0| 771.4952846232337|                 4.0|          610| 635.1152246349538|\n",
      "|          product_id|      |      |      |32951|   string|         32951|fffe9eeff12fcbd74...|                  |00066f42aeeb9f300...|            0|                  |\n",
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df = create_feature_summary(features_df)\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARIS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cloud_config= {\n",
    "        'secure_connect_bundle': db_bundle_path\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider(user_name, password)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()\n",
    "\n",
    "row = session.execute(\"select * from nosql1.users_by_city\").one()\n",
    "if row:\n",
    "    print(row[0])\n",
    "else:\n",
    "    print(\"An error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('summary', 'string'), ('product_id', 'string'), ('product_category_name', 'string'), ('product_name_lenght', 'string'), ('product_description_lenght', 'string'), ('product_photos_qty', 'string'), ('product_weight_g', 'string'), ('product_length_cm', 'string'), ('product_height_cm', 'string'), ('product_width_cm', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print([col for col in summary_df.dtypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporpyspark.sql.catalogca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Couldn't find nosql1.feature_summary or any similarly named keyspace and table pairs;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5e8d62ceb0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m summary_df.write.format(\"org.apache.spark.sql.cassandra\"\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               ).options(table=\"feature_summary\", keyspace=\"nosql1\").save()\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Couldn't find nosql1.feature_summary or any similarly named keyspace and table pairs;"
     ]
    }
   ],
   "source": [
    "summary_df.write.format(\"org.apache.spark.sql.cassandra\"\n",
    "                        ).mode('append'\n",
    "                              ).options(table=\"feature_summary\", keyspace=\"nosql1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('product_id', 'string'),\n",
       " ('product_category_name', 'string'),\n",
       " ('product_name_lenght', 'double'),\n",
       " ('product_description_lenght', 'double'),\n",
       " ('product_photos_qty', 'double'),\n",
       " ('product_weight_g', 'double'),\n",
       " ('product_length_cm', 'double'),\n",
       " ('product_height_cm', 'double'),\n",
       " ('product_width_cm', 'double')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|       summary|         review_id|            order_id|        review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|         count|            105188|              102859|              102692|               12176|                 41868|               96025|                  96002|\n",
      "|          mean|               4.5|                 0.0|   4.071667849964501|3.155449396525236...|  1.111111111111111...|                null|                   null|\n",
      "|        stddev|0.7071067811865476|                 0.0|   1.386648877434681|5.616554832455847E11|              Infinity|                null|                   null|\n",
      "|           min|                 \"|                    |                   \"|                    |                      | FOI A MINHA PRIM...|    POIS NÃO CUMPREM...|\n",
      "|           max|     🤙🏼👏🏼👏🏼\"|visando sempre o ...|seria mais coeren...|                 🔟 |  😡😡😡😡😡👎👎👎👎👎|veio bem embalada...|              70 + R$15|\n",
      "|           25%|               4.0|                 0.0|                 4.0|                 5.0|                   8.0|                null|                   null|\n",
      "|           50%|               4.0|                 0.0|                 5.0|                10.0|                  10.0|                null|                   null|\n",
      "|           75%|               5.0|                 0.0|                 5.0|                10.0|                  10.0|                null|                   null|\n",
      "|     data_type|            string|              string|              string|              string|                string|              string|                 string|\n",
      "| missing_count|                 1|                2330|                2497|               93013|                 63321|                9164|                   9187|\n",
      "|distinct_count|            103956|              100562|                2567|                5044|                 36663|                 725|                  95072|\n",
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_count|\n",
      "+-----------+\n",
      "|      32951|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.agg(countDistinct(\"product_id\").alias(\"total_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "|             summary|count|                 max|              mean|                 min|            stddev|\n",
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "|  product_photos_qty|32341|                   9|2.1889861166939797|                   1|1.7367656379315435|\n",
      "|product_category_...|32341|utilidades_domest...|                  |agro_industria_e_...|                  |\n",
      "|    product_weight_g|32949|                 998|2276.4724877841513|                   0| 4282.038730977024|\n",
      "|    product_width_cm|32949|                  98|23.196728277034204|                  10|12.079047453227794|\n",
      "| product_name_lenght|32341|                   9| 48.47694876472589|                  10|10.245740725237287|\n",
      "|   product_height_cm|32949|                  99|16.937661234028347|                  10|13.637554061749569|\n",
      "|   product_length_cm|32949|                  99| 30.81507784758263|                  10|16.914458054065953|\n",
      "|product_descripti...|32341|                 999| 771.4952846232337|                 100| 635.1152246349538|\n",
      "|          product_id|32951|fffe9eeff12fcbd74...|                  |00066f42aeeb9f300...|                  |\n",
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_features_df = get_transpose_df(features_df.describe(), features_df.columns, \"summary\")\n",
    "describe_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_distinct_count(col_name_str, features_df):\n",
    "    distinct_count = features_df.select(col_name_str).distinct().count()\n",
    "    return distinct_count \n",
    "\n",
    "# get_distinct_count_udf = udf(lambda z: get_distinct_count(z),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.select(\"product_photos_qty\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_features_df.select(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[product_id: string, product_category_name: string, product_name_lenght: string, product_description_lenght: string, product_photos_qty: string, product_weight_g: string, product_length_cm: string, product_height_cm: string, product_width_cm: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-ec6ff5785f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribe_features_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_distinct_count_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df.select(col(\"Seqno\"), \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     convertUDF(col(\"Name\")).alias(\"Name\") ) \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    .show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[product_id: string, product_category_name: string, product_name_lenght: string, product_description_lenght: string, product_photos_qty: string, product_weight_g: string, product_length_cm: string, product_height_cm: string, product_width_cm: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "describe_features_df.withColumn(\"type\", get_distinct_count_udf(col(\"summary\"), features_df))#.show(truncate=False)\n",
    "\n",
    "# df.select(col(\"Seqno\"), \\\n",
    "#     convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "#    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# def main():\n",
    "#     DataList = [(\"Shirts\", 10, 13, 34, 10), (\"Trousers\", 11, 2, 30, 20), (\"Pants\", 70, 43, 24, 60), (\"Sweater\", 101, 44, 54, 80)]\n",
    "#     productQtyDF  = spark.createDataFrame(DataList, [\"Products\", \"Small\", \"Medium\", \"Large\", \"ExLarge\"])\n",
    "#     productQtyDF.show()\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "#     #|Products|Small|Medium|Large|ExLarge|\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "#     #|  Shirts|   10|    13|   34|     10|\n",
    "#     #|Trousers|   11|     2|   30|     20|\n",
    "#     #|   Pants|   70|    43|   24|     60|\n",
    "#     #| Sweater|  101|    44|   54|     80|\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "\n",
    "#     productTypeDF = TransposeDF(productQtyDF, [\"Small\", \"Medium\", \"Large\", \"ExLarge\"], \"Products\")\n",
    "#     productTypeDF.show(truncate= False)\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "#     #|Products|Pants|Shirts|Sweater|Trousers|\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "#     #|Medium  |43   |13    |44     |2       |\n",
    "#     #|Small   |70   |10    |101    |11      |\n",
    "#     #|ExLarge |60   |10    |80     |20      |\n",
    "#     #|Large   |24   |34    |54     |30      |\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32952"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|       _c0|                 _c1|                _c2|                 _c3|               _c4|             _c5|              _c6|              _c7|             _c8|\n",
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|product_id|product_category_...|product_name_lenght|product_descripti...|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_venv",
   "language": "python",
   "name": "fastai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
